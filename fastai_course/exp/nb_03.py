
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/03_minibatch_training_jax.ipynb

from exp.nb_02 import *
import fastax.layers.combinators as cb
import fastax.utils

def accuracy(out, yb): return (np.argmax(out, dim=1)==yb).float().mean()

from torch import optim

class Dataset():
    def __init__(self, x, y): self.x,self.y = x,y
    def __len__(self): return len(self.x)
    def __getitem__(self, i): return self.x[i],self.y[i]

class Sampler():
    def __init__(self, ds, bs, shuffle=False, key=None):
        self.n,self.bs,self.shuffle, self.key = len(ds),bs,shuffle, key

    def __iter__(self):
        self.idxs = jax.random.shuffle(key, np.arange(self.n)) if self.shuffle else np.arange(self.n)
        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]

def collate(b):
    xs,ys = zip(*b)
    return np.stack(xs),np.stack(ys)

class DataLoader():
    def __init__(self, ds, sampler, collate_fn=collate):
        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn

    def __iter__(self):
        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])

def get_dls(train_ds, valid_ds, bs, **kwargs):
    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, **kwargs))